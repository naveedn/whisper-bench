# Whisper Bench

A comprehensive benchmarking tool for comparing Whisper speech-to-text implementations with VAD (Voice Activity Detection) support.

## Features

- ğŸ¯ **4 Whisper Implementations**: OpenAI Whisper, faster-whisper, mlx-whisper, lightning-whisper-mlx
- ğŸ” **VAD Support**: Process only speech segments to avoid hallucinations on silent audio
- â±ï¸ **Performance Metrics**: Timing, throughput, and realtime factor analysis
- ğŸ“Š **Fair Comparisons**: Consistent configuration across all models (greedy decoding, temperature=0)
- ğŸ“ **Quality Analysis**: Save individual transcripts for manual comparison
- ğŸ›ï¸ **Simple Setup**: Drop audio files in `inputs/` folder and run

## Quick Start

### 1. Installation
```bash
git clone <repo-url>
cd whisper-bench
uv sync  # Install dependencies
```

### 2. Add Your Audio
```bash
# Place your files in the inputs folder:
inputs/
â”œâ”€â”€ my_audio.wav                    # Your audio file
â””â”€â”€ my_audio_VAD_timestamps.json    # Optional: VAD timestamps for better quality
```

### 3. Run Benchmark
```bash
uv run python main.py
```

## Input Files

### Audio Files
Supported formats: `.wav`, `.mp3`, `.flac`

### VAD Timestamps (Optional)
JSON file with speech segments to avoid processing silent sections:
```json
[
  {
    "start_sec": 10.5,
    "end_sec": 15.8, 
    "duration_sec": 5.3
  }
]
```

Benefits of VAD:
- âœ… Avoids Whisper hallucinations on long silent sections
- âœ… More accurate quality assessment 
- âœ… Focus transcription on actual speech content

## Configuration

All models use **identical settings** for fair comparison:
- **Greedy decoding** (beam_size=1) - most restrictive common denominator
- **Temperature=0** - deterministic output
- **Word timestamps=True** - detailed timing information
- **No conditioning** on previous text

## Output

Results are saved to `benchmark_results/`:
```
benchmark_results/
â”œâ”€â”€ benchmark_results_YYYYMMDD_HHMMSS.json  # Detailed metrics
â”œâ”€â”€ benchmark_summary_YYYYMMDD_HHMMSS.txt   # Human-readable summary
â””â”€â”€ transcripts/                            # Individual model outputs
    â”œâ”€â”€ whisper/
    â”œâ”€â”€ faster_whisper/ 
    â”œâ”€â”€ mlx_whisper/
    â””â”€â”€ lightning_whisper_mlx/
```

## Model Comparison

| Model | Speed | Platform | Beam Search |
|-------|-------|----------|-------------|
| **OpenAI Whisper** | Moderate | All | âœ… |
| **faster-whisper** | Fast | All | âœ… |
| **mlx-whisper** | Fast | Apple Silicon | âŒ (greedy only) |
| **lightning-whisper-mlx** | Very Fast | Apple Silicon | âŒ (greedy only) |

**Note**: All models now use greedy decoding (beam_size=1) to ensure fair comparison.

## Example Output

```
ğŸ¯ Found audio file in inputs folder: podcast.wav
ğŸ” Found VAD timestamps file: podcast_VAD_timestamps.json
ğŸ” Enabling VAD timestamp processing to avoid hallucinations on silent sections

Starting benchmark with 4 total tests...
Models: ['whisper', 'faster-whisper', 'mlx-whisper', 'lightning-whisper-mlx']
--------------------------------------------------------------------------------

ğŸµ Processing: podcast.wav
  ğŸ“ Model size: base
    ğŸ”„ [1/4] whisper... âœ… 45.2s (12.3x realtime)
    ğŸ”„ [2/4] faster-whisper... âœ… 32.1s (17.3x realtime)
    ğŸ”„ [3/4] mlx-whisper... âœ… 18.7s (29.7x realtime)
    ğŸ”„ [4/4] lightning-whisper-mlx... âœ… 15.3s (36.4x realtime)

================================================================================
BENCHMARK COMPLETE!
================================================================================
```

## Requirements

- Python 3.11+
- Audio files in supported formats
- For MLX models: Apple Silicon Mac
- For VAD processing: `librosa` and `soundfile` (auto-installed)

## Architecture

```
whisper-bench/
â”œâ”€â”€ main.py              # Entry point with inputs/ folder detection
â”œâ”€â”€ config.py            # Global settings for fair comparison
â”œâ”€â”€ benchmarks/          # Model implementations
â”‚   â”œâ”€â”€ base.py         # Abstract base class with VAD support
â”‚   â”œâ”€â”€ openai_whisper.py
â”‚   â”œâ”€â”€ faster_whisper.py  
â”‚   â”œâ”€â”€ mlx_whisper.py
â”‚   â””â”€â”€ lightning_whisper_mlx.py
â”œâ”€â”€ inputs/              # Place your audio + VAD files here
â””â”€â”€ benchmark_results/   # Generated results and transcripts
```

## Troubleshooting

- **MLX models fail**: Requires Apple Silicon Mac
- **No audio found**: Place files in `inputs/` folder
- **VAD not working**: Ensure `librosa` installed with `uv sync`
- **Out of memory**: Use shorter audio files or smaller model sizes